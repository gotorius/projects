{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ad0384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Val 重複: 0件\n",
      "Train-Test 重複: 33件\n",
      "Class distribution - 0: 131072, 1: 131072\n",
      "Class distribution - 0: 16399, 1: 16369\n",
      "Class distribution - 0: 16391, 1: 16377\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221743/2782016007.py:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  grad_scaler = torch.cuda.amp.GradScaler()\n",
      "Epoch 1/30:   0%|          | 0/4096 [00:00<?, ?it/s]/tmp/ipykernel_221743/2782016007.py:210: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/30:   0%|          | 0/4096 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([64])) must be the same as input size (torch.Size([64, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 212\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast():\n\u001b[32m    211\u001b[39m     outputs = model(imgs)\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m    214\u001b[39m grad_scaler.scale(loss).backward()\n\u001b[32m    215\u001b[39m grad_scaler.unscale_(optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mBalancedFocalLoss.forward\u001b[39m\u001b[34m(self, inputs, targets)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# ラベル平滑化\u001b[39;00m\n\u001b[32m     92\u001b[39m     targets = (\u001b[32m1\u001b[39m - \u001b[38;5;28mself\u001b[39m.smoothing) * targets + \u001b[38;5;28mself\u001b[39m.smoothing / inputs.size(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     bce_loss = nn.functional.binary_cross_entropy_with_logits(\n\u001b[32m     95\u001b[39m         inputs, targets, reduction=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     96\u001b[39m     )\n\u001b[32m     97\u001b[39m     pt = torch.exp(-bce_loss)\n\u001b[32m     98\u001b[39m     loss = (\u001b[38;5;28mself\u001b[39m.alpha * (\u001b[32m1\u001b[39m-pt)**\u001b[38;5;28mself\u001b[39m.gamma * bce_loss).mean()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/functional.py:3624\u001b[39m, in \u001b[36mbinary_cross_entropy_with_logits\u001b[39m\u001b[34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[39m\n\u001b[32m   3621\u001b[39m     reduction_enum = _Reduction.get_enum(reduction)\n\u001b[32m   3623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target.size() == \u001b[38;5;28minput\u001b[39m.size()):\n\u001b[32m-> \u001b[39m\u001b[32m3624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3625\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3626\u001b[39m     )\n\u001b[32m   3628\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.binary_cross_entropy_with_logits(\n\u001b[32m   3629\u001b[39m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[32m   3630\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Target size (torch.Size([64])) must be the same as input size (torch.Size([64, 2]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 1. データ整合性チェック関数\n",
    "def check_data_leak(train_path, val_path, test_path):\n",
    "    \"\"\"データセット間の重複をチェック\"\"\"\n",
    "    with h5py.File(train_path, 'r') as train, \\\n",
    "         h5py.File(val_path, 'r') as val, \\\n",
    "         h5py.File(test_path, 'r') as test:\n",
    "        \n",
    "        train_data = train['x'][:]\n",
    "        val_data = val['x'][:]\n",
    "        test_data = test['x'][:]\n",
    "        \n",
    "        # メモリ効率的なハッシュ比較\n",
    "        train_hashes = {hash(x.tobytes()) for x in train_data}\n",
    "        val_hashes = {hash(x.tobytes()) for x in val_data}\n",
    "        test_hashes = {hash(x.tobytes()) for x in test_data}\n",
    "        \n",
    "        print(f\"Train-Val 重複: {len(train_hashes & val_hashes)}件\")\n",
    "        print(f\"Train-Test 重複: {len(train_hashes & test_hashes)}件\")\n",
    "\n",
    "# データリークチェック実行\n",
    "check_data_leak('camelyonpatch_level_2_split_train_x.h5',\n",
    "               'valid_x_uncompressed.h5',\n",
    "               'camelyonpatch_level_2_split_test_x.h5')\n",
    "\n",
    "# 2. データ拡張と正規化\n",
    "mean = [0.702, 0.538, 0.691]\n",
    "std = [0.238, 0.279, 0.213]\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# 3. 癌特異的データ拡張\n",
    "class CancerSpecificAugment:\n",
    "    def __call__(self, img, label):\n",
    "        if label == 1:  # 癌クラスのみに適用\n",
    "            img = transforms.functional.adjust_sharpness(img, 1.8)  # 強度を少し下げる\n",
    "            img = transforms.functional.adjust_contrast(img, 1.1)\n",
    "        return img\n",
    "\n",
    "# 4. カスタムデータセット\n",
    "class PCamDataset(Dataset):\n",
    "    def __init__(self, h5_x_path, h5_y_path, transform=None):\n",
    "        self.x_h5 = h5py.File(h5_x_path, 'r')['x']\n",
    "        self.y_h5 = h5py.File(h5_y_path, 'r')['y']\n",
    "        self.transform = transform\n",
    "        self.augment = CancerSpecificAugment()\n",
    "        \n",
    "        # ラベルデータをNumPy配列に変換してからbincountを適用\n",
    "        y_data = np.array(self.y_h5[:])\n",
    "        self.class_counts = np.bincount(y_data.flatten())  # flatten()で1次元化\n",
    "        \n",
    "        print(f\"Class distribution - 0: {self.class_counts[0]}, 1: {self.class_counts[1]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_h5)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.x_h5[idx]\n",
    "        label = self.y_h5[idx].item()  # .item()でPythonスカラーに変換\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            image = self.augment(image, label)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# 5. 改良版Focal Loss\n",
    "class BalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # ラベル平滑化\n",
    "        targets = (1 - self.smoothing) * targets + self.smoothing / inputs.size(1)\n",
    "        \n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        loss = (self.alpha * (1-pt)**self.gamma * bce_loss).mean()\n",
    "        return loss\n",
    "\n",
    "# 6. データセット初期化\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "    transforms.RandomErasing(p=0.1, scale=(0.02, 0.1))\n",
    "])\n",
    "\n",
    "train_dataset = PCamDataset('camelyonpatch_level_2_split_train_x.h5',\n",
    "                          'camelyonpatch_level_2_split_train_y.h5',\n",
    "                          transform=train_transform)\n",
    "\n",
    "val_dataset = PCamDataset('valid_x_uncompressed.h5',\n",
    "                         'valid_y_uncompressed.h5',\n",
    "                         transform=transform_test)\n",
    "\n",
    "test_dataset = PCamDataset('camelyonpatch_level_2_split_test_x.h5',\n",
    "                         'camelyonpatch_level_2_split_test_y.h5',\n",
    "                         transform=transform_test)\n",
    "\n",
    "# 7. データローダー設定\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                         shuffle=True, num_workers=4, pin_memory=True,\n",
    "                         drop_last=True)  # 最後の不完全なバッチを除外\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
    "                       shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 8. モデル構築\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# クラス重みの計算 (クラス不均衡対策)\n",
    "class_weights = torch.tensor([\n",
    "    1.0,  # Class 0\n",
    "    train_dataset.class_counts[0] / train_dataset.class_counts[1]  # Class 1\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# 事前学習層を凍結\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 最終層の再設計\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(256, 2)\n",
    ")\n",
    "\n",
    "# 勾配計算を有効にする層\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 9. 損失関数と最適化\n",
    "criterion = BalancedFocalLoss(alpha=0.8, gamma=2.0, smoothing=0.05)\n",
    "\n",
    "# 層別学習率\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.layer4.parameters(), 'lr': 5e-5}\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# 学習率スケジューラ\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=1e-5,\n",
    "    max_lr=1e-4,\n",
    "    step_size_up=2000,\n",
    "    cycle_momentum=False\n",
    ")\n",
    "\n",
    "# 10. 訓練ループ\n",
    "best_val_acc = 0.0\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "grad_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/30')\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 混合精度訓練\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        grad_scaler.scale(loss).backward()\n",
    "        grad_scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        grad_scaler.step(optimizer)\n",
    "        grad_scaler.update()\n",
    "        \n",
    "        # メトリクス計算\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Loss': f\"{train_loss/(pbar.n+1):.4f}\",\n",
    "            'Acc': f\"{100.*correct/total:.2f}%\",\n",
    "            'LR': f\"{optimizer.param_groups[0]['lr']:.1e}\"\n",
    "        })\n",
    "    \n",
    "    # 検証フェーズ\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'\\nValidation: Acc={val_acc:.2f}%, Loss={val_loss:.4f}')\n",
    "    \n",
    "    # ベストモデル保存\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        no_improve = 0\n",
    "        print(f\"New best model saved! (Accuracy: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# 11. 最終評価\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# テストセット評価\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_acc = 100. * test_correct / test_total\n",
    "print(f'\\nFinal Test Accuracy: {test_acc:.2f}%')\n",
    "\n",
    "# クラスごとの精度\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# 混同行列\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Class 0', 'Class 1'],\n",
    "            yticklabels=['Class 0', 'Class 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fdd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, Shape: torch.Size([3, 96, 96])\n",
      "Pixel range: -3.244 to 1.138\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m check_sample(train_dataset, \u001b[32m0\u001b[39m)\n\u001b[32m     11\u001b[39m check_sample(val_dataset, \u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mcheck_sample\u001b[39m\u001b[34m(dataset, index)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPixel range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg.min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg.max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m plt.imshow(img.permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m).clamp(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m      7\u001b[39m plt.title(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m plt.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
