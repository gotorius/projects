{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46d2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from collections import Counter\n",
    "\n",
    "class PCamDataset(Dataset):\n",
    "    def __init__(self, h5_x_path, h5_y_path, transform=None):\n",
    "        self.x_h5 = h5py.File(h5_x_path, 'r')\n",
    "        self.y_h5 = h5py.File(h5_y_path, 'r')\n",
    "        self.transform = transform\n",
    "        self.length = len(self.y_h5['y'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.x_h5['x'][idx]  # shape: (96, 96, 3), dtype: uint8\n",
    "        label = self.y_h5['y'][idx].item()\n",
    "\n",
    "\n",
    "        # numpy → PIL → transform\n",
    "        image = image.astype(np.uint8)\n",
    "        image = transforms.ToPILImage()(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(96, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# データセットとデータローダーの設定 (あなたのコードをそのまま使用)\n",
    "train_dataset = PCamDataset('camelyonpatch_level_2_split_train_x.h5',\n",
    "                           'camelyonpatch_level_2_split_train_y.h5',\n",
    "                           transform=transform_train)\n",
    "\n",
    "test_dataset = PCamDataset('camelyonpatch_level_2_split_test_x.h5',\n",
    "                          'camelyonpatch_level_2_split_test_y.h5',\n",
    "                          transform=transform_test)\n",
    "\n",
    "val_dataset = PCamDataset('valid_x_uncompressed.h5', 'valid_y_uncompressed.h5', transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e39b61",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ラベルをすべて取得\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m labels = [train_dataset[i][\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# クラスごとの件数をカウント\u001b[39;00m\n\u001b[32m      7\u001b[39m class_counts = Counter(labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ラベルをすべて取得\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m labels = [train_dataset[i][\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset))]\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# クラスごとの件数をカウント\u001b[39;00m\n\u001b[32m      7\u001b[39m class_counts = Counter(labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mPCamDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     35\u001b[39m image = transforms.ToPILImage()(image)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform(image)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = t(img)\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/transforms/transforms.py:1280\u001b[39m, in \u001b[36mColorJitter.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m   1278\u001b[39m         img = F.adjust_saturation(img, saturation_factor)\n\u001b[32m   1279\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m fn_id == \u001b[32m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1280\u001b[39m         img = F.adjust_hue(img, hue_factor)\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/transforms/functional.py:968\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    966\u001b[39m     _log_api_usage_once(adjust_hue)\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil.adjust_hue(img, hue_factor)\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.adjust_hue(img, hue_factor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/transforms/_functional_pil.py:109\u001b[39m, in \u001b[36madjust_hue\u001b[39m\u001b[34m(img, hue_factor)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mI\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mF\u001b[39m\u001b[33m\"\u001b[39m}:\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m h, s, v = img.convert(\u001b[33m\"\u001b[39m\u001b[33mHSV\u001b[39m\u001b[33m\"\u001b[39m).split()\n\u001b[32m    111\u001b[39m np_h = np.array(h, dtype=np.uint8)\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# This will over/underflow, as desired\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/PIL/Image.py:1143\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m   1140\u001b[39m     dither = Dither.FLOYDSTEINBERG\n\u001b[32m   1142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1143\u001b[39m     im = \u001b[38;5;28mself\u001b[39m.im.convert(mode, dither)\n\u001b[32m   1144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m   1145\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1146\u001b[39m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# ラベルをすべて取得\n",
    "labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "\n",
    "# クラスごとの件数をカウント\n",
    "class_counts = Counter(labels)\n",
    "\n",
    "# 合計数\n",
    "total = sum(class_counts.values())\n",
    "\n",
    "# 出力\n",
    "for cls in sorted(class_counts):\n",
    "    count = class_counts[cls]\n",
    "    ratio = count / total * 100\n",
    "    print(f\"Class {cls}: {count} samples ({ratio:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469be18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:28<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1463 Acc: 0.8897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1510 Acc: 0.8902\n",
      "Epoch 1/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:31<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1281 Acc: 0.9071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1372 Acc: 0.9043\n",
      "Epoch 2/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:35<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1213 Acc: 0.9130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1543 Acc: 0.8859\n",
      "Epoch 3/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:32<00:00, 15.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1133 Acc: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1460 Acc: 0.8989\n",
      "Epoch 4/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:31<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0946 Acc: 0.9350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1862 Acc: 0.8838\n",
      "Epoch 5/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:37<00:00, 14.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.1048 Acc: 0.9256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1714 Acc: 0.8810\n",
      "Epoch 6/29\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4096/4096 [04:34<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0854 Acc: 0.9416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 44.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Loss: 0.1433 Acc: 0.9011\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:11<00:00, 43.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Performance ===\n",
      "Test Accuracy: 0.8883\n",
      "[[15048  1343]\n",
      " [ 2316 14061]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8666    0.9181    0.8916     16391\n",
      "           1     0.9128    0.8586    0.8849     16377\n",
      "\n",
      "    accuracy                         0.8883     32768\n",
      "   macro avg     0.8897    0.8883    0.8882     32768\n",
      "weighted avg     0.8897    0.8883    0.8882     32768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# ========================\n",
    "# データ変換\n",
    "# ========================\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ========================\n",
    "# データローダー\n",
    "# ========================\n",
    "train_dataset = PCamDataset('camelyonpatch_level_2_split_train_x.h5', 'camelyonpatch_level_2_split_train_y.h5', transform=transform_train)\n",
    "val_dataset   = PCamDataset('valid_x_uncompressed.h5', 'valid_y_uncompressed.h5', transform=transform_test)\n",
    "test_dataset  = PCamDataset('camelyonpatch_level_2_split_test_x.h5', 'camelyonpatch_level_2_split_test_y.h5', transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# ========================\n",
    "# モデル構築\n",
    "# ========================\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 2)\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Pretrained ResNet50 & modify final layer\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # PCamは2クラス\n",
    "model = model.to(device)\n",
    "\n",
    "# ========================\n",
    "# Focal Loss\n",
    "# ========================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=2, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        logp = self.ce(inputs, targets)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = self.alpha * (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()\n",
    "\n",
    "# ========================\n",
    "# 環境設定\n",
    "# ========================\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = FocalLoss(alpha=2, gamma=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# ========================\n",
    "# 学習関数\n",
    "# ========================\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}\\n----------')\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            dataloader = train_loader if phase == 'train' else val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in tqdm(dataloader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_val_acc:\n",
    "                    best_val_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), f'model_valacc_{best_val_acc:.4f}.pth')\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= 5:\n",
    "                        print(\"Early stopping\")\n",
    "                        model.load_state_dict(best_model_wts)\n",
    "                        return model\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# ========================\n",
    "# 学習実行\n",
    "# ========================\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=30)\n",
    "\n",
    "# ========================\n",
    "# テスト評価 + クラス別分析\n",
    "# ========================\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        preds = (probs[:, 1] > 0.4).long()  # 閾値調整でRecall改善\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# 精度・詳細評価\n",
    "print(\"\\n=== Test Performance ===\")\n",
    "print(f\"Test Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84d6f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/512 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [02:19<00:00,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TTA Test Performance ===\n",
      "Test Accuracy: 0.8958\n",
      "[[15128  1263]\n",
      " [ 2152 14225]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8755    0.9229    0.8986     16391\n",
      "           1     0.9185    0.8686    0.8928     16377\n",
      "\n",
      "    accuracy                         0.8958     32768\n",
      "   macro avg     0.8970    0.8958    0.8957     32768\n",
      "weighted avg     0.8969    0.8958    0.8957     32768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tta_transforms = [\n",
    "    transforms.Compose([  # オリジナル\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transforms.Compose([  # 左右反転\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transforms.Compose([  # 上下反転\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    transforms.Compose([  # 回転90°\n",
    "        transforms.RandomRotation(degrees=(90, 90)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "]\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "# 正規化済みの逆変換（ToPILImage用）\n",
    "def denormalize_for_pil(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        B = images.size(0)\n",
    "\n",
    "        tta_preds = []\n",
    "\n",
    "        for t in tta_transforms:\n",
    "            aug_imgs = []\n",
    "            for i in range(B):\n",
    "                img = denormalize_for_pil(images[i].cpu())\n",
    "                img_pil = to_pil(img)\n",
    "                img_aug = t(img_pil)  # TTA変換適用\n",
    "                aug_imgs.append(img_aug)\n",
    "            batch = torch.stack(aug_imgs).to(device)\n",
    "            outputs = model(batch)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            tta_preds.append(probs)\n",
    "\n",
    "        mean_probs = torch.stack(tta_preds).mean(dim=0)\n",
    "        preds = (mean_probs[:, 1] > 0.4).long()\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(mean_probs[:, 1].cpu().numpy())\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"\\n=== TTA Test Performance ===\")\n",
    "print(f\"Test Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9301d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, images, labels, epsilon):\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.clone().detach().to(device)\n",
    "    images.requires_grad = True\n",
    "\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # FGSM perturbation: sign(∇_x loss)\n",
    "    perturbed_images = images + epsilon * images.grad.sign()\n",
    "    perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
    "    return perturbed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb0fb44",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m epsilon = \u001b[32m0.03\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model.eval()\n\u001b[32m      4\u001b[39m adv_imgs = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(imgs, labels):\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "epsilon = 0.03\n",
    "\n",
    "model.eval()\n",
    "adv_imgs = []\n",
    "\n",
    "for img, label in zip(imgs, labels):\n",
    "    img = img.unsqueeze(0).to(device)      # (C, H, W) → (1, C, H, W)\n",
    "    label = label.unsqueeze(0).to(device)  # ラベルもバッチ化\n",
    "\n",
    "    adv_img = fgsm_attack(model, img, label, epsilon)\n",
    "    adv_imgs.append(adv_img.detach().cpu())  # detachとCPUに移して蓄積\n",
    "\n",
    "torch.cuda.empty_cache()  # メモリの断片化も軽減\n",
    "adv_imgs = torch.cat(adv_imgs)  # (N, C, H, W)\n",
    "\n",
    "# ここから敵対的画像を用いた推論・正解率計算\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for adv_img, label in zip(adv_imgs, labels):\n",
    "        adv_img = adv_img.unsqueeze(0).to(device)\n",
    "        label = label.unsqueeze(0).to(device)\n",
    "\n",
    "        output = model(adv_img)\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += (pred == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "acc = 100 * correct / total\n",
    "print(f'Adversarial Test Accuracy (FGSM, ε={epsilon}): {acc:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
