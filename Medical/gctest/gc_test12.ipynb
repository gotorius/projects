{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34823117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcb535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Dataset定義（PCam用）\n",
    "# ----------------------------\n",
    "class PCamDataset(Dataset):\n",
    "    def __init__(self, h5_x_path, h5_y_path=None, transform=None):\n",
    "        self.x_path = h5_x_path\n",
    "        self.y_path = h5_y_path\n",
    "        self.transform = transform\n",
    "        self.has_labels = h5_y_path is not None  # ← 修正\n",
    "\n",
    "        with h5py.File(h5_x_path, 'r') as x_file:\n",
    "            self.length = len(x_file['x'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.x_path, 'r') as x_file:\n",
    "            image = x_file['x'][idx]\n",
    "\n",
    "        image = transforms.ToPILImage()(image.astype(np.uint8))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.has_labels:\n",
    "            with h5py.File(self.y_path, 'r') as y_file:\n",
    "                label = y_file['y'][idx].astype(np.float32)\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f0a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# データ変換\n",
    "# ----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(96, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_val_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d2d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# データローダ-\n",
    "# ----------------------------\n",
    "train_dataset = PCamDataset('camelyonpatch_level_2_split_train_x.h5',\n",
    "                          'camelyonpatch_level_2_split_train_y.h5',\n",
    "                          transform=transform_train)\n",
    "\n",
    "val_dataset = PCamDataset('valid_x_uncompressed.h5',\n",
    "                         'valid_y_uncompressed.h5',\n",
    "                         transform=transform_val_test)\n",
    "\n",
    "test_dataset = PCamDataset('camelyonpatch_level_2_split_test_x.h5',\n",
    "                         'camelyonpatch_level_2_split_test_y.h5',  # テストラベルを追加\n",
    "                         transform=transform_val_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1038a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gotou/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gotou/miniconda3/envs/myenv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.3481 Acc: 0.8496\n",
      "val Loss: 0.3421 Acc: 0.8500\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.2936 Acc: 0.8777\n",
      "val Loss: 0.3827 Acc: 0.8467\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.2714 Acc: 0.8878\n",
      "val Loss: 0.3403 Acc: 0.8591\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.2594 Acc: 0.8945\n",
      "val Loss: 0.3280 Acc: 0.8695\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.2503 Acc: 0.8990\n",
      "val Loss: 0.3044 Acc: 0.8713\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2429 Acc: 0.9020\n",
      "val Loss: 0.3549 Acc: 0.8488\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2395 Acc: 0.9040\n",
      "val Loss: 0.4228 Acc: 0.8341\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2028 Acc: 0.9210\n",
      "val Loss: 0.3147 Acc: 0.8733\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.1938 Acc: 0.9252\n",
      "val Loss: 0.3175 Acc: 0.8782\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.1878 Acc: 0.9278\n",
      "val Loss: 0.3315 Acc: 0.8729\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.1842 Acc: 0.9295\n",
      "val Loss: 0.3647 Acc: 0.8631\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.1809 Acc: 0.9308\n",
      "val Loss: 0.3186 Acc: 0.8800\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.1783 Acc: 0.9322\n",
      "val Loss: 0.3234 Acc: 0.8770\n",
      "\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.1771 Acc: 0.9326\n",
      "val Loss: 0.3385 Acc: 0.8689\n",
      "\n",
      "Epoch 14/24\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    126\u001b[39m exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=\u001b[32m7\u001b[39m, gamma=\u001b[32m0.1\u001b[39m)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# モデル訓練\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m model = train_model(\n\u001b[32m    130\u001b[39m     model, \n\u001b[32m    131\u001b[39m     criterion, \n\u001b[32m    132\u001b[39m     optimizer, \n\u001b[32m    133\u001b[39m     exp_lr_scheduler, \n\u001b[32m    134\u001b[39m     num_epochs=\u001b[32m25\u001b[39m\n\u001b[32m    135\u001b[39m )\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# テストデータで評価\u001b[39;00m\n\u001b[32m    138\u001b[39m test_acc = evaluate_model(model, test_loader)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m phase == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     60\u001b[39m         loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m         optimizer.step()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# 統計情報\u001b[39;00m\n\u001b[32m     64\u001b[39m running_loss += loss.item() * inputs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:137\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m opt = opt_ref()\n\u001b[32m    136\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func.\u001b[34m__get__\u001b[39m(opt, opt.\u001b[34m__class__\u001b[39m)(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = func(*args, **kwargs)\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    211\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    213\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    214\u001b[39m         group,\n\u001b[32m    215\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         state_steps,\n\u001b[32m    221\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     adam(\n\u001b[32m    224\u001b[39m         params_with_grad,\n\u001b[32m    225\u001b[39m         grads,\n\u001b[32m    226\u001b[39m         exp_avgs,\n\u001b[32m    227\u001b[39m         exp_avg_sqs,\n\u001b[32m    228\u001b[39m         max_exp_avg_sqs,\n\u001b[32m    229\u001b[39m         state_steps,\n\u001b[32m    230\u001b[39m         amsgrad=group[\u001b[33m\"\u001b[39m\u001b[33mamsgrad\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    231\u001b[39m         has_complex=has_complex,\n\u001b[32m    232\u001b[39m         beta1=beta1,\n\u001b[32m    233\u001b[39m         beta2=beta2,\n\u001b[32m    234\u001b[39m         lr=group[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    235\u001b[39m         weight_decay=group[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    236\u001b[39m         eps=group[\u001b[33m\"\u001b[39m\u001b[33meps\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    237\u001b[39m         maximize=group[\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    238\u001b[39m         foreach=group[\u001b[33m\"\u001b[39m\u001b[33mforeach\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    239\u001b[39m         capturable=group[\u001b[33m\"\u001b[39m\u001b[33mcapturable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    240\u001b[39m         differentiable=group[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    241\u001b[39m         fused=group[\u001b[33m\"\u001b[39m\u001b[33mfused\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    242\u001b[39m         grad_scale=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrad_scale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    243\u001b[39m         found_inf=\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfound_inf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    244\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/adam.py:784\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    782\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m func(\n\u001b[32m    785\u001b[39m     params,\n\u001b[32m    786\u001b[39m     grads,\n\u001b[32m    787\u001b[39m     exp_avgs,\n\u001b[32m    788\u001b[39m     exp_avg_sqs,\n\u001b[32m    789\u001b[39m     max_exp_avg_sqs,\n\u001b[32m    790\u001b[39m     state_steps,\n\u001b[32m    791\u001b[39m     amsgrad=amsgrad,\n\u001b[32m    792\u001b[39m     has_complex=has_complex,\n\u001b[32m    793\u001b[39m     beta1=beta1,\n\u001b[32m    794\u001b[39m     beta2=beta2,\n\u001b[32m    795\u001b[39m     lr=lr,\n\u001b[32m    796\u001b[39m     weight_decay=weight_decay,\n\u001b[32m    797\u001b[39m     eps=eps,\n\u001b[32m    798\u001b[39m     maximize=maximize,\n\u001b[32m    799\u001b[39m     capturable=capturable,\n\u001b[32m    800\u001b[39m     differentiable=differentiable,\n\u001b[32m    801\u001b[39m     grad_scale=grad_scale,\n\u001b[32m    802\u001b[39m     found_inf=found_inf,\n\u001b[32m    803\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/adam.py:592\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    590\u001b[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     bias_correction1 = [\n\u001b[32m    593\u001b[39m         \u001b[32m1\u001b[39m - beta1 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    594\u001b[39m     ]\n\u001b[32m    595\u001b[39m     bias_correction2 = [\n\u001b[32m    596\u001b[39m         \u001b[32m1\u001b[39m - beta2 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    597\u001b[39m     ]\n\u001b[32m    599\u001b[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/adam.py:593\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    590\u001b[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    592\u001b[39m     bias_correction1 = [\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m         \u001b[32m1\u001b[39m - beta1 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    594\u001b[39m     ]\n\u001b[32m    595\u001b[39m     bias_correction2 = [\n\u001b[32m    596\u001b[39m         \u001b[32m1\u001b[39m - beta2 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    597\u001b[39m     ]\n\u001b[32m    599\u001b[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/optim/optimizer.py:106\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.item() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネル (Kernel) がクラッシュしました。\n",
      "\u001b[1;31mエラーの原因を特定するには、セル内のコードを確認してください。\n",
      "\u001b[1;31m詳細については<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a>をクリックします。\n",
      "\u001b[1;31m詳細については、Jupyter <a href='command:jupyter.viewOutput'>ログ</a> を参照してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# デバイス設定\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# モデル定義 (ResNet50)\n",
    "# ----------------------------\n",
    "def create_model():\n",
    "    model = resnet50(pretrained=True)  # ImageNet事前学習モデル\n",
    "    \n",
    "    # 最終層をカスタマイズ (2クラス分類)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),  # 過学習防止\n",
    "        nn.Linear(num_ftrs, 2)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 訓練関数\n",
    "# ----------------------------\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 各エポックの訓練と検証フェーズ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 訓練モード\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # 評価モード\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # データをイテレート\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long().squeeze()\n",
    "\n",
    "                # 勾配をゼロに\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝播\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # 訓練時は逆伝播+最適化\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 統計情報\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # 最高精度モデルを深層コピー\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # ベストモデル重みをロード\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# 評価関数\n",
    "# ----------------------------\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).long().squeeze()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            corrects += torch.sum(preds == labels.data)\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = corrects.double() / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    return accuracy\n",
    "\n",
    "# ----------------------------\n",
    "# メイン実行\n",
    "# ----------------------------\n",
    "if __name__ == '__main__':\n",
    "    # モデル作成\n",
    "    model = create_model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 損失関数とオプティマイザ\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 学習率スケジューラ (7エポックごとに学習率を1/10に減少)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    # モデル訓練\n",
    "    model = train_model(\n",
    "        model, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        exp_lr_scheduler, \n",
    "        num_epochs=25\n",
    "    )\n",
    "    \n",
    "    # テストデータで評価\n",
    "    test_acc = evaluate_model(model, test_loader)\n",
    "    \n",
    "    # 95%以上を達成したか確認\n",
    "    if test_acc >= 0.95:\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"Successfully achieved 95%+ accuracy!\")\n",
    "    else:\n",
    "        print(\"Did not reach target accuracy. Consider tuning hyperparameters.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
